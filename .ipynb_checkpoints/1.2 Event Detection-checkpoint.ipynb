{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import public packages and functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# inserting the lib folder to the compiler\n",
    "sys.path.insert(0, './lib')\n",
    "sys.path.insert(0, './utils/')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from lib_event import EVENTS\n",
    "from lib_data import DATA_IO\n",
    "\n",
    "import utils_plotting_accelerometer, utils_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CURR   = os.path.abspath(os.curdir)    # current code\n",
    "PATH        = (str(Path(PATH_CURR).parent)) # data repository: upper directory where datasets situated\n",
    "PATH_DATA   = PATH + \"\\\\data\"               # the recordings data directory\n",
    "PATH_EVENTS = PATH + \"\\\\events\"\n",
    "SUB_LIST    = utils_misc.get_SUB_list(PATH_DATA) # get the SUB id list which we have a recording of them"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Events with a duration shorter than 200ms were dropped due to unstable statistics and PSD estimation in further analyses. Additionally, the events in the accelerometer and electrophysiological recordings were separately identified due to the different sampling frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ACCELEROMETER EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Detect Accelerometer Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for SUB in SUB_LIST:\n",
    "    EVENTS_HISTORY    = EVENTS(PATH, SUB, 'acc_right')\n",
    "    df_SUB_acc_events = EVENTS_HISTORY.get_event_dataframe()\n",
    "    df_SUB_acc_events = df_SUB_acc_events[df_SUB_acc_events.duration >= 0.2]\n",
    "    df_SUB_acc_events.to_csv(DATA_IO.path_events + \"SUB_\"+ SUB +\"_EVENTS_ACC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Merge Accelerometer Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accelerometer_events = pd.DataFrame()\n",
    "event_files             = utils_misc.get_files_with_specific_format(PATH_EVENTS, suffix=\".csv\")\n",
    "\n",
    "for file in event_files:\n",
    "    if(\"ACC\" in file):\n",
    "        if(len(df_accelerometer_events)==0): # if the dataframe is empty\n",
    "            df_accelerometer_events = pd.read_csv(PATH_EVENTS + \"\\\\\" + file)\n",
    "        else:\n",
    "            df_accelerometer_events = pd.concat([df_accelerometer_events, pd.read_csv(PATH_EVENTS + \"\\\\\" + file)], ignore_index=True)\n",
    "\n",
    "# Apply the function to each row and create a new column\n",
    "df_accelerometer_events.to_csv(DATA_IO.path_events + \"EVENTS_ACC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LFP/ECOG EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Detect LFP Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... pickle loading: C:\\Users\\a.kaymak\\Desktop\\Papers\\2025 Parkinson STN-DBS Dyskinesia LFP-ECG\\files\\data\\sub-019\\019_mergedData_v4.0_lfp_right.P\n",
      "... task periods were defined\n",
      "... events were categorized\n",
      "... dyskinesia evaluation was collected\n",
      "... event loading completed\n",
      "--------------------------------------------------------------------\n",
      "EVENT HISTORY: SUB-020\n",
      "... loading started\n"
     ]
    }
   ],
   "source": [
    "for SUB in SUB_LIST:\n",
    "    EVENTS_HISTORY    = EVENTS(PATH, SUB, 'lfp_right')\n",
    "    df_SUB_lfp_events = EVENTS_HISTORY.get_event_dataframe()\n",
    "    df_SUB_lfp_events = df_SUB_lfp_events[df_SUB_lfp_events.duration >= 0.2]\n",
    "    df_SUB_lfp_events.to_csv(DATA_IO.path_events + \"SUB_\"+ SUB +\"_EVENTS_LFP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Merge LFP/ECoG Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lfp_events           = pd.DataFrame()\n",
    "event_files             = utils_misc.get_files_with_specific_format(PATH_EVENTS, suffix=\".csv\")\n",
    "\n",
    "for file in event_files:\n",
    "    if(\"LFP\" in file):\n",
    "        if(len(df_lfp_events)==0): # if the dataframe is empty\n",
    "            df_lfp_events = pd.read_csv(PATH_EVENTS + \"\\\\\" + file)\n",
    "        else:\n",
    "            df_lfp_events = pd.concat([df_lfp_events, pd.read_csv(PATH_EVENTS + \"\\\\\" + file)], ignore_index=True)\n",
    "\n",
    "df_lfp_events.to_csv(DATA_IO.path_events + \"EVENTS_LFP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
